{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Variational Autoencoders (VAEs) and Transposed Convolutions\"  \n",
        " This notebook provides an in-depth walkthrough of autoencoders and variational autoencoders (VAEs) using PyTorch.\n",
        " It is designed as a teaching tool for HCC students with detailed explanations and visualizations.\n",
        " ## Topics covered:\n",
        "- Simple Autoencoders\n",
        "- Variational Autoencoders (VAEs)\n",
        "- Convolutional VAE reconstructions\n",
        "- Latent space interpolation\n",
        "- Visualizations of the reparameterization trick, loss functions, and network architectures"
      ],
      "metadata": {
        "id": "qJt55ua2J3Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Autoencoders learn compressed representations of data and then reconstruct the input.\n",
        "Variational Autoencoders (VAEs) add a probabilistic twist by mapping inputs to a latent distribution (mean and log-variance).\n",
        "This continuous latent space allows for generating new samples and smooth interpolation between examples.\n",
        "Key components:\n",
        "- Encoder: Maps input data to a latent probability distribution.\n",
        "- Latent Space: A structured, continuous representation.\n",
        "- Decoder: Reconstructs the input from a sampled latent vector.\n",
        "The reparameterization trick (z = mean + exp(0.5 * logvar) * epsilon, where epsilon ~ N(0,1)) allows gradients to flow during training."
      ],
      "metadata": {
        "id": "9FaPnlX4OTLN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYUUKV-MJxjS"
      },
      "outputs": [],
      "source": [
        "#Install required packages if needed:\n",
        "!pip install torch torchvision matplotlib numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Autoencoders? (A Gentle Introduction)\n",
        "\n",
        "Imagine you have a picture, and you want to create a smaller, compressed version of it, but still be able to recreate the original image from that smaller version. That's essentially what an autoencoder does!\n",
        "\n",
        "Autoencoders are a type of neural network that learn how to compress data (like images) into a lower-dimensional \"code,\" and then learn how to \"decode\" it back to the original. They're like a fancy compression algorithm that learns what parts of the data are most important.\n",
        "\n",
        "They have two main parts:\n",
        "- **Encoder:** This part takes the input data (e.g., an image) and squeezes it down to a more compact form (the latent space).\n",
        "- **Decoder:** This part takes the compressed data and tries to reconstruct the original input as closely as possible."
      ],
      "metadata": {
        "id": "Mr2lh_frYdv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#For visualization architectures\n",
        "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
        "\n",
        "# Ensure plots show inline in a Jupyter Notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "y9ZE6nChOji5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Autoencoder for MNIST\n",
        "The MNIST images (28x28) are flattened into a 784-dimensional vector.\n",
        "The encoder compresses this into a latent space, and the decoder reconstructs the image.\n",
        "ReLU activations introduce non-linearity and Sigmoid ensures outputs are in the range [0,1]."
      ],
      "metadata": {
        "id": "XVrBwLQ3PWVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Visualization of the Autoencoder\n",
        "The following functions train the autoencoder on the MNIST dataset and visualize original versus reconstructed images."
      ],
      "metadata": {
        "id": "gDAdQkL7Pn2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variational Autoencoder (VAE)\n",
        "In a VAE the encoder outputs two vectors: one for the mean and one for the log-variance.\n",
        "The latent vector is sampled using the reparameterization trick:\n",
        "z = mean + exp(0.5 * logvar) * epsilon (where epsilon is drawn from a standard normal distribution)\n",
        "The loss function is a sum of the reconstruction loss (binary cross-entropy) and the KL divergence.\n"
      ],
      "metadata": {
        "id": "VH_DRAuBQypP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diving into Simple Autoencoders\n",
        "\n",
        "In this section, we will start with a very basic autoencoder to get a feel for how they work.  We'll use the MNIST dataset, which contains images of handwritten digits (0-9). Each image is 28x28 pixels.\n",
        "\n",
        "Here's the basic idea:\n",
        "1. We'll take each 28x28 image and flatten it out into a 784-dimensional vector (28 * 28 = 784).\n",
        "2. The encoder will take this 784-dimensional vector, and compress it into something smaller.\n",
        "3.  The decoder will take that smaller compressed vector and try to reconstruct the original 784-dimensional vector (and thus the original image).\n",
        "\n",
        "This will give us a \"code\" for the image. The better the autoencoder learns, the closer the reconstructed image will be to the original."
      ],
      "metadata": {
        "id": "suxJ0jGvY7WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 2: Simple Autoencoder Class\n",
        "class SimpleAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=128, latent_dim=32):\n",
        "        \"\"\"\n",
        "        A basic autoencoder for MNIST digits.\n",
        "        - input_dim: Size of the flattened MNIST image (28*28 = 784)\n",
        "        - hidden_dim: Number of neurons in the hidden layer\n",
        "        - latent_dim: Size of the latent representation\n",
        "        \"\"\"\n",
        "        super(SimpleAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() > 2:  # Flatten image if needed\n",
        "            x = x.view(x.size(0), -1)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "6xREI7VLRWIJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 3: Create and Print Autoencoder\n",
        "autoencoder = SimpleAutoencoder()\n",
        "print(\"Simple Autoencoder Architecture:\")\n",
        "print(autoencoder)\n"
      ],
      "metadata": {
        "id": "aDax8SL6X0Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 4: Train Autoencoder Function\n",
        "def train_autoencoder(model, epochs=5, batch_size=128, learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    Train the autoencoder on the MNIST dataset.\n",
        "    Returns the trained model and a list of average losses per epoch.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(\"Training on device:\", device)\n",
        "\n",
        "    # Use the training dataset for training\n",
        "    transform = transforms.ToTensor()\n",
        "    train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error for reconstruction\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            data_flat = data.view(data.size(0), -1)\n",
        "            output = model(data_flat)\n",
        "            loss = criterion(output, data_flat)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - Batch {batch_idx} Loss: {loss.item():.6f}\")\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    return model, losses\n"
      ],
      "metadata": {
        "id": "Bwv_TiSQYShS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_autoencoder, training_losses = train_autoencoder(autoencoder, epochs=5)\n",
        "print(\"Training Losses per Epoch:\")\n",
        "print(training_losses)\n"
      ],
      "metadata": {
        "id": "Gn39AIbnPoMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(training_losses, marker='o', linestyle='-')\n",
        "plt.title(\"Training Losses Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TXi0MScuL0LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 5: Autoencoder Reconstruction Visualization\n",
        "def visualize_reconstructions(model, num_examples=10):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed images from the autoencoder.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    transform = transforms.ToTensor()\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=num_examples, shuffle=True)\n",
        "    dataiter = iter(test_loader)\n",
        "    images, _ = next(dataiter)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images_flat = images.view(images.size(0), -1)\n",
        "        reconstructions = model(images_flat).view(images.size())\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(2, num_examples, i+1)\n",
        "        plt.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(2, num_examples, i+num_examples+1)\n",
        "        plt.imshow(reconstructions[i].squeeze().numpy(), cmap=\"gray\")\n",
        "        plt.title(\"Reconstructed\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RngLoyQNYWiM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Limitations of Simple Autoencoders\n",
        "\n",
        "Our simple autoencoder can reconstruct images, but it has limitations:\n",
        "*   **No Control Over Latent Space:** The latent space (the compressed representation) might not be smooth or continuous. This means that small changes in the latent space don't always translate to smooth changes in the output, making it hard to create variations or interpolate between different inputs.\n",
        "*  **Not Generative:** The autoencoder is not a generative model - we cannot sample from the latent space to create new images.\n",
        "\n",
        "This is where Variational Autoencoders come in. VAEs introduce a probabilistic element to the latent space to address these limitations."
      ],
      "metadata": {
        "id": "5U9A7IJcZLtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Variational Autoencoders (VAEs)\n",
        "\n",
        "VAEs improve on simple autoencoders by adding a probabilistic twist. Instead of just compressing an input to a single code, a VAE maps an input to a distribution in the latent space.\n",
        "\n",
        "Think of it this way:\n",
        "*   **Simple Autoencoder:** Learns a single code (a point) in latent space for each input.\n",
        "*   **VAE:** Learns a distribution (a \"cloud\" of points) in latent space for each input. This distribution is characterized by a mean (the center of the cloud) and a log-variance (how spread out the cloud is).\n",
        "\n",
        "This difference is important. By mapping to a distribution, the VAE creates a smoother, more continuous latent space. This has several benefits.\n",
        "1.  **Generation:** We can sample from this latent distribution to generate new data points that are similar to the training data.\n",
        "2.  **Interpolation:** We can move through the latent space and generate a smooth transition of different outputs.\n",
        "\n",
        "The key idea of the VAE is to make the latent space smooth, organized and allow for sampling new data from it."
      ],
      "metadata": {
        "id": "H0IxW0SVZlk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 6: Simple VAE Class\n",
        "class SimpleVAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=32):\n",
        "        \"\"\"\n",
        "        A basic Variational Autoencoder for MNIST digits.\n",
        "        - input_dim: Flattened input size.\n",
        "        - hidden_dim: Number of neurons in hidden layers.\n",
        "        - latent_dim: Dimensionality of the latent space.\n",
        "        \"\"\"\n",
        "        super(SimpleVAE, self).__init__()\n",
        "        # Encoder network\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Two separate linear layers for mean and log-variance\n",
        "        self.fc_mean = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "        # Decoder network\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input into mean and log-variance.\"\"\"\n",
        "        hidden = self.encoder(x)\n",
        "        mean = self.fc_mean(hidden)\n",
        "        logvar = self.fc_logvar(hidden)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        \"\"\"\n",
        "        Sample latent vector z using the reparameterization trick.\n",
        "        z = mean + exp(0.5 * logvar) * epsilon, where epsilon ~ N(0, 1)\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mean + std * epsilon\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Reconstruct input from latent vector z.\"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        mean, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        reconstructed = self.decode(z)\n",
        "        return reconstructed, mean, logvar\n"
      ],
      "metadata": {
        "id": "7JBoILLAYZrH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 7: Create and Print VAE\n",
        "vae_model = SimpleVAE()\n",
        "print(\"Simple VAE Architecture:\")\n",
        "print(vae_model)\n"
      ],
      "metadata": {
        "id": "ct8i7ERQYkcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE Loss Function\n",
        "The VAE loss function consists of:\n",
        "- Reconstruction Loss: Binary cross-entropy between the input and its reconstruction.\n",
        "- KL Divergence: Measures how much the latent distribution diverges from a standard normal distribution.\n",
        "The total loss is given by:\n",
        "Total Loss = Reconstruction Loss + beta * KL Divergence"
      ],
      "metadata": {
        "id": "MrINtSBnRIXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Importance of the Reparameterization Trick\n",
        "\n",
        "The VAE uses the \"reparameterization trick\" to make the training process possible.\n",
        "The VAE wants to sample from the latent space to decode back to an image, but doing so is a non-differentiable operation.\n",
        "\n",
        "The reparameterization trick rewrites the sampling in a way that allows gradients to flow during training. Instead of directly sampling from the latent distribution, the trick samples from a standard normal distribution (N(0,1)) and transforms the random number using the mean and the variance learned from the encoder.  This allows our network to update the encoder based on the loss.\n",
        "\n",
        "The equation for the reparameterization is:\n",
        "`z = mean + exp(0.5 * logvar) * epsilon`\n",
        "where:\n",
        "- `z` is the sampled latent vector\n",
        "- `mean` and `logvar` are outputs of the encoder\n",
        "-  `epsilon` is a random number drawn from N(0,1).\n",
        "\n",
        "This ensures that while we introduce randomness, the randomness is separate from the learnable parameters."
      ],
      "metadata": {
        "id": "BvFDwqUkZ6FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 8: VAE Loss Function\n",
        "def vae_loss(recon_x, x, mean, logvar, beta=1.0):\n",
        "    \"\"\"\n",
        "    Compute the VAE loss.\n",
        "    Total Loss = Reconstruction Loss + beta * KL Divergence.\n",
        "    Reconstruction Loss: Binary cross-entropy.\n",
        "    KL Divergence: Regularizes the latent distribution.\n",
        "    \"\"\"\n",
        "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "    total_loss = recon_loss + beta * kl_loss\n",
        "    return total_loss, recon_loss, kl_loss\n",
        "\n",
        "    # Load a batch of MNIST images for testing\n",
        "transform = transforms.ToTensor()\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=10, shuffle=True)\n",
        "images, labels = next(iter(test_loader))\n",
        "\n",
        "# Flatten images (since our VAE expects a 784-length vector)\n",
        "images = images.view(images.size(0), -1)\n",
        "\n",
        "# Pass through the VAE model (assumes vae_model is defined in Cell 7)\n",
        "recon, mean, logvar = vae_model(images)\n",
        "\n",
        "# Compute the loss using the vae_loss function\n",
        "total_loss, recon_loss, kl_loss = vae_loss(recon, images, mean, logvar)\n",
        "\n",
        "print(\"Total Loss:\", total_loss.item())\n",
        "print(\"Reconstruction Loss:\", recon_loss.item())\n",
        "print(\"KL Divergence:\", kl_loss.item())\n",
        "\n"
      ],
      "metadata": {
        "id": "5nPL8ID_Ys2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional VAE Visualization Functions\n",
        "These functions visualize the reconstructions from a convolutional VAE and generate new images by sampling from its latent space.\n",
        "(Note: Your convolutional VAE model should have a method called sample for image generation.)"
      ],
      "metadata": {
        "id": "GHW-QDmbRXM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Convolutional VAEs?\n",
        "\n",
        "Our previous VAE examples used fully connected layers, which process the data as a simple list of numbers, ignoring any spatial information. Convolutional layers, on the other hand, take into account the spatial structure in images by looking at small patches in the image at a time.\n",
        "\n",
        "Convolutional VAEs are better suited for images because:\n",
        "1.  They can extract features at multiple scales: this is key to having good image representations.\n",
        "2.  They reduce the number of parameters by sharing weights across the image.\n",
        "3.  They are more efficient with spatial data than fully connected layers.\n",
        "\n",
        "Convolutional VAEs will use transposed convolutions for decoding. Transposed convolutions are sometimes called \"deconvolutions\" but the process is actually the opposite direction of a convolutional layer.  A convolutional layer takes an image and creates a set of feature maps. Transposed convolution takes a set of feature maps and creates an image or a set of feature maps that are larger.\n",
        "\n",
        "In short, they allow us to build VAEs that deal with images much better."
      ],
      "metadata": {
        "id": "u5uerAK_aB3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 9: Convolutional VAE Visualization Functions\n",
        "\n",
        "def visualize_conv_vae_reconstructions(model, num_examples=10):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed images from a convolutional VAE.\n",
        "    (Assumes your model accepts images as input and returns reconstructions.)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load MNIST test dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=num_examples, shuffle=True)\n",
        "    dataiter = iter(test_loader)\n",
        "    images, _ = next(dataiter)\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        # If model returns a tuple (reconstructions, mean, logvar), use the first element.\n",
        "        if isinstance(outputs, tuple):\n",
        "            reconstructions = outputs[0]\n",
        "        else:\n",
        "            reconstructions = outputs\n",
        "\n",
        "    images = images.cpu()\n",
        "    reconstructions = reconstructions.cpu()\n",
        "\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(2, num_examples, i+1)\n",
        "        # Original images: expect shape (1, 28, 28); squeeze to (28,28)\n",
        "        plt.imshow(images[i].squeeze().numpy(), cmap=\"gray\")\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(2, num_examples, i+num_examples+1)\n",
        "        # For reconstructed images, if output is flattened (shape (784,)), reshape to (28,28)\n",
        "        img = reconstructions[i].squeeze()\n",
        "        if img.numel() == 784:\n",
        "            img = img.view(28, 28)\n",
        "        plt.imshow(img.numpy(), cmap=\"gray\")\n",
        "        plt.title(\"Reconstructed\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def generate_images_from_conv_vae(model, num_examples=10):\n",
        "    \"\"\"\n",
        "    Generate new images by sampling from the latent space of a convolutional VAE.\n",
        "    (Your model must implement a 'sample' method for this to work.)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_images = model.sample(num_examples)\n",
        "        generated_images = generated_images.cpu()\n",
        "\n",
        "    plt.figure(figsize=(20, 3))\n",
        "    for i in range(num_examples):\n",
        "        plt.subplot(1, num_examples, i+1)\n",
        "        img = generated_images[i].squeeze()\n",
        "        if img.numel() == 784:\n",
        "            img = img.view(28, 28)\n",
        "        plt.imshow(img.numpy(), cmap=\"gray\")\n",
        "        plt.title(\"Generated\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# For demonstration purposes, we'll use the simple VAE 'vae_model'\n",
        "# Ensure that 'vae_model' is already defined in your notebook.\n",
        "if __name__ == \"__main__\":\n",
        "    test_model = vae_model  # Replace with your conv VAE if available\n",
        "    visualize_conv_vae_reconstructions(test_model)\n",
        "    # If your model has a 'sample' method, you can also run:\n",
        "    # generate_images_from_conv_vae(test_model)\n"
      ],
      "metadata": {
        "id": "k1ryWRIlRcN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Space Interpolation\n",
        "This function selects one example each of two specified digits from MNIST, encodes them, and linearly interpolates between their latent representations.\n",
        "The resulting reconstructions show a smooth transition between the two digits."
      ],
      "metadata": {
        "id": "1UYPny8ARhJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 10: Latent Space Interpolation\n",
        "def interpolate_digits(model, digit1=3, digit2=8, steps=10):\n",
        "    \"\"\"\n",
        "    Interpolate between two digits in the VAE's latent space.\n",
        "    Finds one example for each digit, encodes them, linearly interpolates between their latent vectors,\n",
        "    and decodes the results to show a smooth transition.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    image1, image2 = None, None\n",
        "    for img, label in test_loader:\n",
        "        if label.item() == digit1 and image1 is None:\n",
        "            image1 = img.to(device)\n",
        "        if label.item() == digit2 and image2 is None:\n",
        "            image2 = img.to(device)\n",
        "        if image1 is not None and image2 is not None:\n",
        "            break\n",
        "\n",
        "    if image1 is None or image2 is None:\n",
        "        print(\"Could not find images for the specified digits.\")\n",
        "        return\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image1_flat = image1.view(1, -1)\n",
        "        image2_flat = image2.view(1, -1)\n",
        "        mean1, _ = model.encode(image1_flat)\n",
        "        mean2, _ = model.encode(image2_flat)\n",
        "\n",
        "    interpolated_images = []\n",
        "    for alpha in np.linspace(0, 1, steps):\n",
        "        z = mean1 * (1 - alpha) + mean2 * alpha\n",
        "        with torch.no_grad():\n",
        "            reconstruction = model.decode(z)\n",
        "        interpolated_images.append(reconstruction.view(28, 28).cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(20, 3))\n",
        "    for i, img in enumerate(interpolated_images):\n",
        "        plt.subplot(1, steps, i+1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Interpolation between {digit1} and {digit2}\", fontsize=16)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Pq6bf0WRRnq5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reparameterization Trick Visualization\n",
        "This visualization shows how the encoder produces the mean (μ) and log-variance (log σ²), how epsilon is sampled from a standard normal distribution, and how these combine to form the latent vector z.\n",
        "The diagram emphasizes that the randomness (epsilon) is separate from the network's learnable parameters."
      ],
      "metadata": {
        "id": "czO8rkzjRpiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 11: Reparameterization Trick Visualization\n",
        "def visualize_reparameterization_trick():\n",
        "    \"\"\"\n",
        "    Visualize the reparameterization trick:\n",
        "    - Shows how the encoder outputs the mean (μ) and log-variance (log σ²),\n",
        "      how a random epsilon is sampled, and how they combine to form z.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Define block positions: [x, y, width, height]\n",
        "    encoder_box = [0.1, 0.4, 0.2, 0.3]\n",
        "    mean_box = [0.4, 0.5, 0.1, 0.1]\n",
        "    logvar_box = [0.4, 0.3, 0.1, 0.1]\n",
        "    reparam_box = [0.6, 0.4, 0.15, 0.15]\n",
        "    decoder_box = [0.8, 0.4, 0.2, 0.3]\n",
        "    random_box = [0.6, 0.2, 0.1, 0.1]\n",
        "\n",
        "    # Draw boxes for each component\n",
        "    ax.add_patch(Rectangle(encoder_box[:2], encoder_box[2], encoder_box[3],\n",
        "                             facecolor=\"lightgreen\", alpha=0.7, edgecolor=\"black\"))\n",
        "    ax.add_patch(Rectangle(mean_box[:2], mean_box[2], mean_box[3],\n",
        "                             facecolor=\"coral\", alpha=0.7, edgecolor=\"black\"))\n",
        "    ax.add_patch(Rectangle(logvar_box[:2], logvar_box[2], logvar_box[3],\n",
        "                             facecolor=\"coral\", alpha=0.7, edgecolor=\"black\"))\n",
        "    ax.add_patch(Rectangle(reparam_box[:2], reparam_box[2], reparam_box[3],\n",
        "                             facecolor=\"lightskyblue\", alpha=0.7, edgecolor=\"black\"))\n",
        "    ax.add_patch(Rectangle(decoder_box[:2], decoder_box[2], decoder_box[3],\n",
        "                             facecolor=\"lightgreen\", alpha=0.7, edgecolor=\"black\"))\n",
        "    ax.add_patch(Rectangle(random_box[:2], random_box[2], random_box[3],\n",
        "                             facecolor=\"gold\", alpha=0.7, edgecolor=\"black\"))\n",
        "\n",
        "    # Add labels inside boxes\n",
        "    ax.text(encoder_box[0] + encoder_box[2]/2, encoder_box[1] + encoder_box[3]/2,\n",
        "            \"Encoder\\nNeural Network\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax.text(mean_box[0] + mean_box[2]/2, mean_box[1] + mean_box[3]/2,\n",
        "            \"μ\", ha=\"center\", va=\"center\", fontsize=14)\n",
        "    ax.text(logvar_box[0] + logvar_box[2]/2, logvar_box[1] + logvar_box[3]/2,\n",
        "            \"log σ²\", ha=\"center\", va=\"center\", fontsize=14)\n",
        "    ax.text(reparam_box[0] + reparam_box[2]/2, reparam_box[1] + reparam_box[3]/2,\n",
        "            \"z = μ + σ × ε\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax.text(decoder_box[0] + decoder_box[2]/2, decoder_box[1] + decoder_box[3]/2,\n",
        "            \"Decoder\\nNeural Network\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax.text(random_box[0] + random_box[2]/2, random_box[1] + random_box[3]/2,\n",
        "            \"ε ~ N(0,1)\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "\n",
        "    # Draw arrows indicating the flow\n",
        "    arrow_kwargs = dict(arrowstyle=\"->\", lw=2, color=\"black\")\n",
        "    ax.annotate(\"\", xy=(encoder_box[0], encoder_box[1] + encoder_box[3]/2),\n",
        "                xytext=(encoder_box[0] - 0.1, encoder_box[1] + encoder_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.text(encoder_box[0] - 0.15, encoder_box[1] + encoder_box[3]/2, \"Input\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax.annotate(\"\", xy=(mean_box[0], mean_box[1] + mean_box[3]/2),\n",
        "                xytext=(encoder_box[0] + encoder_box[2], encoder_box[1] + encoder_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(logvar_box[0], logvar_box[1] + logvar_box[3]/2),\n",
        "                xytext=(encoder_box[0] + encoder_box[2], encoder_box[1] + encoder_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(reparam_box[0], reparam_box[1] + reparam_box[3]/2),\n",
        "                xytext=(mean_box[0] + mean_box[2], mean_box[1] + mean_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(reparam_box[0], reparam_box[1] + reparam_box[3]/2),\n",
        "                xytext=(logvar_box[0] + logvar_box[2], logvar_box[1] + logvar_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(reparam_box[0] + reparam_box[2]/2, reparam_box[1]),\n",
        "                xytext=(random_box[0] + random_box[2]/2, random_box[1] + random_box[3]),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(decoder_box[0], decoder_box[1] + decoder_box[3]/2),\n",
        "                xytext=(reparam_box[0] + reparam_box[2], reparam_box[1] + reparam_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.annotate(\"\", xy=(decoder_box[0] + decoder_box[2] + 0.1, decoder_box[1] + decoder_box[3]/2),\n",
        "                xytext=(decoder_box[0] + decoder_box[2], decoder_box[1] + decoder_box[3]/2),\n",
        "                arrowprops=arrow_kwargs)\n",
        "    ax.text(decoder_box[0] + decoder_box[2] + 0.15, decoder_box[1] + decoder_box[3]/2,\n",
        "            \"Output\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "\n",
        "    explanation = (\n",
        "        \"Key Points:\\n\"\n",
        "        \"1. Encoder outputs μ (mean) and log σ² (log-variance).\\n\"\n",
        "        \"2. Convert log σ² to σ using exp(0.5 * log σ²).\\n\"\n",
        "        \"3. Sample ε from N(0,1) and compute z = μ + σ × ε.\\n\"\n",
        "        \"4. Gradients flow through μ and σ during backpropagation.\"\n",
        "    )\n",
        "    ax.text(0.5, 0.85, \"The Reparameterization Trick\", ha=\"center\", fontsize=16, weight=\"bold\")\n",
        "    ax.text(0.5, 0.02, explanation, ha=\"center\", va=\"bottom\", fontsize=11,\n",
        "            bbox=dict(facecolor=\"white\", alpha=0.7, boxstyle=\"round,pad=0.5\"))\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "%matplotlib inline\n",
        "interpolate_digits(vae_model, digit1=3, digit2=8, steps=10)\n",
        "visualize_reparameterization_trick()"
      ],
      "metadata": {
        "id": "AGHd8QChRzNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE Loss Function Visualization\n",
        "This diagram explains the two main components of the VAE loss:\n",
        "- Reconstruction Loss: How closely the output matches the input.\n",
        "- KL Divergence: How far the latent distribution is from a standard normal distribution."
      ],
      "metadata": {
        "id": "IPaj60w6R347"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the VAE Training Process\n",
        "\n",
        "When training the VAE, we are trying to minimize the total loss, which is made of the reconstruction loss and the KL divergence. The reconstruction loss is essentially like a reconstruction error. The KL divergence term regularizes the latent space by penalizing distributions that are very different from a standard normal distribution, making the latent space continuous and nicely structured.\n",
        "\n",
        "During the training of the VAE, it's expected that the reconstruction loss will go down over time and hopefully that the KL divergence also goes down as the model learns to map the input data to the latent space.\n",
        "\n",
        "In practice, you will see that the reconstruction loss usually reaches a lower point than KL divergence, which can be a little harder to minimize."
      ],
      "metadata": {
        "id": "x6rozDbfaZgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 12: VAE Loss Function Visualization\n",
        "def visualize_vae_loss_function():\n",
        "    \"\"\"\n",
        "    Visualize the two components of the VAE loss:\n",
        "      - Reconstruction Loss: How well the output matches the input.\n",
        "      - KL Divergence: How far the latent distribution is from a standard normal distribution.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    ax.add_patch(Rectangle((0.05, 0.5), 0.4, 0.35, facecolor=\"lightblue\", alpha=0.3))\n",
        "    ax.add_patch(Rectangle((0.05, 0.15), 0.4, 0.35, facecolor=\"lightgreen\", alpha=0.3))\n",
        "    ax.add_patch(Rectangle((0.55, 0.15), 0.4, 0.7, facecolor=\"lightsalmon\", alpha=0.3))\n",
        "\n",
        "    ax.text(0.25, 0.9, \"Reconstruction Loss\", ha=\"center\", fontsize=14, weight=\"bold\")\n",
        "    ax.text(0.25, 0.55, \"Measures how well we reconstruct the input\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    ax.text(0.25, 0.5, \"KL Divergence\", ha=\"center\", fontsize=14, weight=\"bold\")\n",
        "    ax.text(0.25, 0.2, \"Measures divergence from a standard normal distribution\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    ax.text(0.75, 0.9, \"Total VAE Loss\", ha=\"center\", fontsize=14, weight=\"bold\")\n",
        "    ax.text(0.75, 0.83, \"Reconstruction Loss + β × KL Divergence\", ha=\"center\", fontsize=12)\n",
        "\n",
        "    recon_details = (\n",
        "        \"Binary Cross-Entropy:\\n\"\n",
        "        \"- Compares each pixel of the original and reconstruction\\n\"\n",
        "        \"- High when differences are large, low when similar\"\n",
        "    )\n",
        "    ax.text(0.25, 0.7, recon_details, ha=\"center\", va=\"center\", fontsize=10,\n",
        "            bbox=dict(facecolor=\"white\", alpha=0.7, boxstyle=\"round,pad=0.3\"))\n",
        "\n",
        "    kl_details = (\n",
        "        \"KL(N(μ, σ²) || N(0, 1)) = -0.5 × sum(1 + log(σ²) - μ² - σ²)\\n\\n\"\n",
        "        \"- Penalizes divergence from N(0,1)\\n\"\n",
        "        \"- Encourages μ near 0 and σ near 1\\n\"\n",
        "        \"- Regularizes the latent space\"\n",
        "    )\n",
        "    ax.text(0.25, 0.35, kl_details, ha=\"center\", va=\"center\", fontsize=10,\n",
        "            bbox=dict(facecolor=\"white\", alpha=0.7, boxstyle=\"round,pad=0.3\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uRMKJsbWR88U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder Architecture Diagram\n",
        "This diagram shows the flow from input to latent space and back to output in a basic autoencoder."
      ],
      "metadata": {
        "id": "D-ua3KeUSE3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 13: Autoencoder Architecture Diagram\n",
        "def plot_autoencoder_architecture():\n",
        "    \"\"\"\n",
        "    Display a diagram showing the flow in a basic autoencoder:\n",
        "    from input to latent space and back to output.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    input_rect = Rectangle((0, 2), 1, 4, color=\"skyblue\", alpha=0.7)\n",
        "    ax.add_patch(input_rect)\n",
        "    ax.text(0.5, 6.5, \"Input (784 neurons for 28x28 image)\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    encoder_rect = Rectangle((2, 2.5), 1, 3, color=\"lightgreen\", alpha=0.7)\n",
        "    ax.add_patch(encoder_rect)\n",
        "    ax.text(2.5, 6.5, \"Encoder Hidden Layer (256 neurons)\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    latent_rect = Rectangle((4, 3), 1, 2, color=\"coral\", alpha=0.7)\n",
        "    ax.add_patch(latent_rect)\n",
        "    ax.text(4.5, 6.5, \"Latent Space (32 neurons)\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    decoder_rect = Rectangle((6, 2.5), 1, 3, color=\"lightgreen\", alpha=0.7)\n",
        "    ax.add_patch(decoder_rect)\n",
        "    ax.text(6.5, 6.5, \"Decoder Hidden Layer (256 neurons)\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    output_rect = Rectangle((8, 2), 1, 4, color=\"lightsalmon\", alpha=0.7)\n",
        "    ax.add_patch(output_rect)\n",
        "    ax.text(8.5, 6.5, \"Output (784 neurons)\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "    arrow1 = FancyArrowPatch((1, 4), (2, 4), arrowstyle=\"->\", mutation_scale=20, color=\"black\")\n",
        "    arrow2 = FancyArrowPatch((3, 4), (4, 4), arrowstyle=\"->\", mutation_scale=20, color=\"black\")\n",
        "    arrow3 = FancyArrowPatch((5, 4), (6, 4), arrowstyle=\"->\", mutation_scale=20, color=\"black\")\n",
        "    arrow4 = FancyArrowPatch((7, 4), (8, 4), arrowstyle=\"->\", mutation_scale=20, color=\"black\")\n",
        "    ax.add_patch(arrow1)\n",
        "    ax.add_patch(arrow2)\n",
        "    ax.add_patch(arrow3)\n",
        "    ax.add_patch(arrow4)\n",
        "\n",
        "    ax.text(1.5, 4.5, \"Compress\", ha=\"center\", fontsize=10)\n",
        "    ax.text(3.5, 4.5, \"Compress\", ha=\"center\", fontsize=10)\n",
        "    ax.text(5.5, 4.5, \"Reconstruct\", ha=\"center\", fontsize=10)\n",
        "    ax.text(7.5, 4.5, \"Reconstruct\", ha=\"center\", fontsize=10)\n",
        "\n",
        "    ax.set_xlim(-1, 10)\n",
        "    ax.set_ylim(0, 8)\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(\"Basic Autoencoder Architecture\", fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5yhJj6mqSTFF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 14: Test / Run desired functions\n",
        "# To train the simple autoencoder:\n",
        "# trained_autoencoder, training_losses = train_autoencoder(autoencoder, epochs=5)\n",
        "# visualize_reconstructions(trained_autoencoder)\n",
        "\n",
        "# To test the VAE:\n",
        "# Use the vae_loss function during training, or visualize reconstructions similarly.\n",
        "\n",
        "# To visualize latent space interpolation:\n",
        "interpolate_digits(vae_model, digit1=3, digit2=8, steps=10)\n",
        "\n",
        "# To visualize the reparameterization trick:\n",
        "visualize_reparameterization_trick()\n",
        "\n",
        "# To visualize the VAE loss function:\n",
        "visualize_vae_loss_function()\n",
        "\n",
        "# To view the autoencoder architecture diagram:\n",
        "plot_autoencoder_architecture()\n"
      ],
      "metadata": {
        "id": "21m-xuJJSTnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In this notebook we explored simple autoencoders and variational autoencoders using PyTorch.\n",
        "We discussed the reparameterization trick, examined the VAE loss function, and visualized both network architectures and latent space interpolations.\n",
        "Experiment with the code, adjust hyperparameters, and add further visualizations to deepen your understanding.\n"
      ],
      "metadata": {
        "id": "cIXXKDKoSZyJ"
      }
    }
  ]
}